{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6222b2f4-9a50-495f-846b-4f39a459cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory (where the notebook is located)\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, os.pardir))\n",
    "\n",
    "# Set the parent directory as the working directory\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "# Optionally, you can add the parent directory to the system path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Print the current working directory to verify\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14eb7f7f-c09a-4dfc-afad-55987116b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:In order to use the mouse gastrulation seqFISH datsets, please install squidpy (see https://github.com/scverse/squidpy).\n",
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 0\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "  new_rank_zero_deprecation(\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.\n",
      "  return new_rank_zero_deprecation(*args, **kwargs)\n",
      "WARNING:root:In order to use sagenet models, please install pytorch geometric (see https://pytorch-geometric.readthedocs.io) and \n",
      " captum (see https://github.com/pytorch/captum).\n",
      "WARNING:root:mvTCR is not installed. To use mvTCR models, please install it first using \"pip install mvtcr\"\n",
      "WARNING:root:multigrate is not installed. To use multigrate models, please install it first using \"pip install multigrate\".\n"
     ]
    }
   ],
   "source": [
    "# import interpretable_ssl.datasets.dataset\n",
    "# import interpretable_ssl.trainers.swav\n",
    "# importlib.reload(interpretable_ssl.datasets.dataset)\n",
    "# importlib.reload(interpretable_ssl.trainers.swav)\n",
    "\n",
    "from interpretable_ssl.trainers.swav import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e493f6e0-0af0-4a20-bbee-3b8efa305307",
   "metadata": {},
   "source": [
    "# check swav related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c31974-6aa2-465d-8989-30e6c666254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new base init\n",
      "loading data\n"
     ]
    }
   ],
   "source": [
    "t = SwAV(augmentaition_type = 'scanpy_knn', dimensionality_reduction = 'pca', debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2012d0d3-333c-4c30-832d-27ccf7b8c415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09/28/24 12:12:29 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 09/28/24 12:12:29 - 0:00:00 - all_latent: None\n",
      "                                     augmentaition_type: scanpy_knn\n",
      "                                     augmentation_type: knn\n",
      "                                     base_lr: 4.8\n",
      "                                     batch_size: 512\n",
      "                                     batch_size_version: 2\n",
      "                                     cell_type_key: cell_type\n",
      "                                     checkpoint_freq: 25\n",
      "                                     condition_key: study\n",
      "                                     crops_for_assign: [0, 1]\n",
      "                                     custom_cross_val: False\n",
      "                                     cvae_loss_scaler: 0.0001\n",
      "                                     cvae_reg: 0\n",
      "                                     dataset: pbmc-immune\n",
      "                                     dataset_id: pbmc-immune\n",
      "                                     debug: True\n",
      "                                     default_values: {'dataset_id': 'pbmc-immune', 'model_name_version': 5.0, 'num_prototypes': 128, 'hidden_dim': 64, 'latent_dims': 8, 'batch_size_version': 2, 'batch_size': 512, 'fine_tuning_epochs': 0, 'custom_cross_val': False, 'description': '', 'experiment_name': '', 'condition_key': 'study', 'cell_type_key': 'cell_type', 'linear_eval': False, 'only_eval': False, 'use_early_stopping': False, 'debug': False, 'pretraining_epochs': 300, 'training_type': 'pretrain', 'dump_name_version': 4, 'nmb_crops': [8], 'augmentation_type': 'knn', 'size_crops': [224], 'min_scale_crops': [0.14], 'max_scale_crops': [1], 'crops_for_assign': [0, 1], 'temperature': 0.1, 'epsilon': 0.05, 'sinkhorn_iterations': 3, 'feat_dim': 8, 'queue_length': 0, 'epoch_queue_starts': 15, 'base_lr': 4.8, 'final_lr': 0, 'freeze_prototypes_niters': 313, 'wd': 1e-06, 'warmup_epochs': 10, 'start_warmup': 0, 'cvae_reg': 0, 'dist_url': 'env://', 'world_size': -1, 'rank': 0, 'local_rank': 0, 'workers': 10, 'checkpoint_freq': 25, 'use_fp16': False, 'sync_bn': 'pytorch', 'syncbn_process_group_size': 8, 'seed': 31, 'model': '', 'optimizer': '', 'lr_schedule': '', 'queue': None, 'train_loader': '', 'training_stats': '', 'device': 'cuda', 'freezable_prototypes': False, 'cvae_loss_scaler': 0.0001, 'prot_decoding_loss_scaler': 5, 'hidden_mlp': 1024, 'swav_dim': 64, 'use_projector': False, 'model_version': 2, 'train_decoder': False, 'longest_path': 3, 'dimensionality_reduction': None, 'k_neighbors': 10}\n",
      "                                     description: None\n",
      "                                     device: cuda\n",
      "                                     dimensionality_reduction: pca\n",
      "                                     dist_url: env://\n",
      "                                     dump_checkpoints: /home/icb/fatemehs.hashemig/models//pbmc-immune/swav-all-loss_iloss5_closs0.0001_num-prot-128_latent8-bs512_aug-knn8_ep0.05_dimensionality_reduction-pca/checkpoints\n",
      "                                     dump_name_version: 4\n",
      "                                     dump_path: /home/icb/fatemehs.hashemig/models//pbmc-immune/swav-all-loss_iloss5_closs0.0001_num-prot-128_latent8-bs512_aug-knn8_ep0.05_dimensionality_reduction-pca\n",
      "                                     epoch_queue_starts: 15\n",
      "                                     epsilon: 0.05\n",
      "                                     experiment_name: swav-all-loss_iloss5_closs0.0001\n",
      "                                     feat_dim: 8\n",
      "                                     final_lr: 0\n",
      "                                     fine_tuning_epochs: 0\n",
      "                                     finetune_ds: None\n",
      "                                     finetuning: False\n",
      "                                     fold: None\n",
      "                                     freezable_prototypes: False\n",
      "                                     freeze_prototypes_niters: 313\n",
      "                                     hidden_dim: 64\n",
      "                                     hidden_mlp: 1024\n",
      "                                     input_dim: 4000\n",
      "                                     k_neighbors: 10\n",
      "                                     latent_dims: 8\n",
      "                                     linear_eval: False\n",
      "                                     local_rank: 0\n",
      "                                     longest_path: 3\n",
      "                                     lr_schedule: None\n",
      "                                     max_scale_crops: [1]\n",
      "                                     min_scale_crops: [0.14]\n",
      "                                     model: None\n",
      "                                     model_name_version: 5.0\n",
      "                                     model_version: 2\n",
      "                                     nmb_crops: [8]\n",
      "                                     nmb_prototypes: 128\n",
      "                                     num_prototypes: 128\n",
      "                                     only_eval: False\n",
      "                                     optimizer: None\n",
      "                                     original_ref: None\n",
      "                                     partial_ref: None\n",
      "                                     pretraining_epochs: 300\n",
      "                                     prot_decoding_loss_scaler: 5\n",
      "                                     query: pbmc-immune\n",
      "                                     query_latent: None\n",
      "                                     queue: None\n",
      "                                     queue_length: 0\n",
      "                                     rank: 0\n",
      "                                     ref: pbmc-immune\n",
      "                                     ref_latent: None\n",
      "                                     seed: 31\n",
      "                                     semi_supervised: False\n",
      "                                     sinkhorn_iterations: 3\n",
      "                                     size_crops: [224]\n",
      "                                     start_warmup: 0\n",
      "                                     swav_dim: 64\n",
      "                                     sync_bn: pytorch\n",
      "                                     syncbn_process_group_size: 8\n",
      "                                     temperature: 0.1\n",
      "                                     train_decoder: False\n",
      "                                     train_loader: None\n",
      "                                     training_stats: None\n",
      "                                     training_type: pretrain\n",
      "                                     use_early_stopping: False\n",
      "                                     use_fp16: False\n",
      "                                     use_projector: False\n",
      "                                     use_projector_out: False\n",
      "                                     warmup_epochs: 10\n",
      "                                     wd: 1e-06\n",
      "                                     workers: 10\n",
      "                                     world_size: -1\n",
      "INFO - 09/28/24 12:12:29 - 0:00:00 - The experiment will be stored in /home/icb/fatemehs.hashemig/models//pbmc-immune/swav-all-loss_iloss5_closs0.0001_num-prot-128_latent8-bs512_aug-knn8_ep0.05_dimensionality_reduction-pca\n",
      "                                     \n",
      "\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby('conditions_combined').first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:156: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby(condition_keys).first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:164: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  adata.obs[cell_type_key][self.labeled_indices_]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary:\n",
      " \tNum conditions: [3]\n",
      " \tEmbedding dim: [10]\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 64 10\n",
      "\tMean/Var Layer in/out: 64 8\n",
      "Decoder Architecture:\n",
      "\tFirst Layer in, out and cond:  8 64 10\n",
      "\tOutput Layer in/out:  64 4000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "INFO - 09/28/24 12:12:31 - 0:00:01 - Building data done with 29137 images loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swav model with l2n init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09/28/24 12:12:32 - 0:00:03 - SwavModel(\n",
      "                                       (scpoli_model): scpoli(\n",
      "                                         (embeddings): ModuleList(\n",
      "                                           (0): Embedding(3, 10, max_norm=1.0)\n",
      "                                         )\n",
      "                                         (encoder): Encoder(\n",
      "                                           (FC): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=4000, out_features=64, bias=True)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=64, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (mean_encoder): Linear(in_features=64, out_features=8, bias=True)\n",
      "                                           (log_var_encoder): Linear(in_features=64, out_features=8, bias=True)\n",
      "                                         )\n",
      "                                         (decoder): Decoder(\n",
      "                                           (FirstL): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=8, out_features=64, bias=False)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=64, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (HiddenL): Sequential()\n",
      "                                           (mean_decoder): Sequential(\n",
      "                                             (0): Linear(in_features=64, out_features=4000, bias=True)\n",
      "                                             (1): Softmax(dim=-1)\n",
      "                                           )\n",
      "                                         )\n",
      "                                       )\n",
      "                                     )\n",
      "INFO - 09/28/24 12:12:32 - 0:00:03 - Building model done.\n",
      "INFO - 09/28/24 12:12:32 - 0:00:03 - Building optimizer done.\n",
      "INFO - 09/28/24 12:12:32 - 0:00:03 - no mixed precision\n"
     ]
    }
   ],
   "source": [
    "t.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b906611-ebaf-4070-b3ea-3d412583deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 09/28/24 12:12:32 - 0:00:03 - Starting to build k-nearest neighbors graph.\n",
      "INFO - 09/28/24 12:12:32 - 0:00:03 - Constructing NearestNeighbors with k=11.\n",
      "INFO - 09/28/24 12:14:13 - 0:01:44 - k-nearest neighbors graph construction completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.585, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.244, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.375, 0.000, 0.000, 0.000,  ..., 1.316, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.424, 0.424, 0.000, 0.000, 0.000,  ..., 1.136, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.087, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.390, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.375, 0.000, 0.000, 0.000,  ..., 1.316, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.069, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000]]),\n",
       " 'labeled': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]], dtype=torch.float64),\n",
       " 'sizefactor': tensor([301.664, 221.887, 245.109, 217.227, 203.039, 208.005, 245.109, 220.434]),\n",
       " 'batch': tensor([[0],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]]),\n",
       " 'combined_batch': tensor([0, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'celltypes': tensor([[2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.train_ds[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27c0556-35cb-489b-8af9-a5e91a7fa9d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[0.000, 0.000, 1.128, 1.983, 0.000, 0.000, 0.000,  ..., 2.117, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.690, 0.000, 0.000, 0.404, 0.000,  ..., 1.249, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.690],\n",
       "         [0.000, 0.000, 0.975, 0.000, 0.000, 0.000, 0.000,  ..., 1.358, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.030, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.405, 0.000, 0.000, 0.000, 0.000,  ..., 0.916, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 1.096, 0.335, 0.000, 0.000, 0.000,  ..., 1.332, 0.000,\n",
       "          0.000, 0.000, 0.335, 0.335, 0.000],\n",
       "         [0.000, 0.000, 0.957, 0.000, 0.000, 0.000, 0.000,  ..., 0.589, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         ...,\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.602, 0.000,  ..., 1.552, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.322, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.761, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.322, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.665, 0.000, 0.000, 0.000,  ..., 1.343, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.244, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.000, 0.000, 0.292, 0.000,  ..., 1.479, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.000, 0.326, 0.000, 0.000, 0.000,  ..., 0.933, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "         [0.000, 0.000, 0.256, 0.000, 0.000, 0.000, 0.000,  ..., 0.628, 0.000,\n",
       "          0.000, 0.000, 0.000, 0.000, 0.000]]),\n",
       " 'labeled': tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]], dtype=torch.float64),\n",
       " 'sizefactor': tensor([337.201, 230.497, 219.800, 229.369, 215.259, 231.276, 228.525, 224.098,\n",
       "         181.557, 165.032, 157.679, 173.028, 165.032, 162.801, 165.032, 157.650,\n",
       "         301.664, 237.006, 253.251, 242.128, 221.887, 238.806, 229.763, 231.527]),\n",
       " 'batch': tensor([[0],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [0],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [0],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]]),\n",
       " 'combined_batch': tensor([0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'celltypes': tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2],\n",
       "         [2]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.train_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc30ca62-bc87-432d-8b2d-7959611b364d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [16384000], which does not match the required output shape [512, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [512, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[0.000, 0.000, 0.000, 1.883, 0.000, 0.000, 0.000,  ..., 1.050, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.580, 0.000, 0.000, 0.000,  ..., 0.717, 0.000,\n",
       "           0.000, 0.000, 0.233, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.354, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.354, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.543, 0.308,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.354, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.354, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.314, 0.000, 0.000, 0.000,  ..., 1.044, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.314, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.627, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 1.249, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.306, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.631, 0.000, 0.000, 0.000,  ..., 0.631, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.306, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.543, 0.308,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.631, 0.000, 0.000, 0.000,  ..., 0.631, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.627, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.046, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.136, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.313, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.880, 0.000, 0.000, 0.000,  ..., 0.534, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.302, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.499, 0.000, 0.000, 0.000,  ..., 0.830, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.714, 0.000, 0.420, 0.000,  ..., 0.420, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.499, 0.000, 0.000, 0.000,  ..., 0.830, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.426, 0.000, 0.000, 0.000,  ..., 0.426, 0.000,\n",
       "           0.426, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.489, 0.000, 0.000, 0.000,  ..., 0.489, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.154, 0.154, 0.000, 0.404, 0.000,  ..., 0.154, 0.000,\n",
       "           0.000, 0.000, 0.404, 0.154, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.231, 0.000, 0.083, 0.000,  ..., 0.231, 0.160,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.231, 0.000, 0.083, 0.000,  ..., 0.231, 0.160,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.111, 0.000, 0.111, 0.000,  ..., 0.210, 0.111,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.080, 0.080, 0.000, 0.154, 0.000, 0.080, 0.000,  ..., 0.080, 0.000,\n",
       "           0.000, 0.000, 0.080, 0.154, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.104, 0.000, 0.000, 0.000,  ..., 0.104, 0.000,\n",
       "           0.000, 0.104, 0.104, 0.199, 0.000]],\n",
       " \n",
       "         [[0.000, 0.524, 0.000, 2.226, 1.121, 0.000, 0.000,  ..., 1.874, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.260, 0.000, 0.000,  ..., 1.695, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.440, 0.000, 0.244, 0.000,  ..., 2.031, 0.244,\n",
       "           0.000, 0.000, 0.000, 0.244, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.440, 0.000, 0.244, 0.000,  ..., 2.031, 0.244,\n",
       "           0.000, 0.000, 0.000, 0.244, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.876, 0.182, 0.182, 0.000,  ..., 1.723, 0.182,\n",
       "           0.000, 0.000, 0.182, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.876, 0.182, 0.182, 0.000,  ..., 1.723, 0.182,\n",
       "           0.000, 0.000, 0.182, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.191, 0.191, 0.000, 0.000, 0.000,  ..., 1.962, 0.191,\n",
       "           0.000, 0.000, 0.191, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.260, 0.000, 0.000,  ..., 1.695, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.079, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.580, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.367, 0.000, 0.000, 0.000,  ..., 0.847, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.321, 0.000, 0.000, 0.000,  ..., 0.758, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.238, 0.000, 0.000, 0.000,  ..., 0.851, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.372, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.251, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.284, 0.000, 0.000, 0.000,  ..., 0.284, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.499, 0.000, 0.000, 0.000,  ..., 0.830, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.627, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.495, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.495, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 1.583, 0.000, 0.000, 0.000,  ..., 1.135, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.305, 0.000, 0.000, 0.000,  ..., 0.305, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.000, 0.000, 0.650, 0.000, 0.000, 0.000, 0.000,  ..., 0.650, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.580, 0.000, 0.000, 0.000,  ..., 0.717, 0.000,\n",
       "           0.000, 0.000, 0.233, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.346, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.373, 0.000, 0.000,  ..., 0.645, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.627, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.499, 0.000, 0.000, 0.000,  ..., 0.830, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.321, 0.000, 0.000, 0.000,  ..., 0.564, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.313, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.067, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.346, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.470, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.313, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.499, 0.000, 0.000, 0.000,  ..., 0.830, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.238, 0.000, 0.000, 0.000,  ..., 0.851, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.314, 0.000, 0.000, 0.000,  ..., 1.044, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.314, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.372, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.790, 0.000, 0.000, 0.000,  ..., 1.528, 0.000,\n",
       "           0.000, 0.000, 0.790, 0.000, 1.528],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.306, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.238, 0.000, 0.000, 0.000,  ..., 0.851, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.305, 0.000, 0.000, 0.000,  ..., 0.305, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.347, 0.000, 0.000,  ..., 0.604, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.627, 0.000, 0.000, 0.000,  ..., 0.898, 0.000,\n",
       "           0.000, 0.000, 0.255, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.499, 0.000, 0.000, 0.000,  ..., 0.830, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.305, 0.000, 0.000, 0.000,  ..., 0.305, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.139, 0.139, 0.000, 0.000,  ..., 0.261, 0.261,\n",
       "           0.000, 0.000, 0.000, 0.261, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.119,\n",
       "           0.119, 0.000, 0.000, 0.119, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.107, 0.000, 0.000, 0.000,  ..., 0.203, 0.107,\n",
       "           0.203, 0.000, 0.000, 0.203, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.095, 0.095, 0.000,  ..., 0.000, 0.095,\n",
       "           0.095, 0.000, 0.000, 0.095, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.105, 0.000, 0.000, 0.000,  ..., 0.201, 0.288,\n",
       "           0.000, 0.000, 0.105, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.222, 0.117, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.117, 0.000, 0.000],\n",
       "          [0.080, 0.080, 0.000, 0.154, 0.000, 0.080, 0.000,  ..., 0.080, 0.000,\n",
       "           0.000, 0.000, 0.080, 0.154, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.719, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.374, 0.000, 0.000, 0.000,  ..., 0.646, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.354, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.354, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.346, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.824, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.467, 0.000, 0.467, 0.000,  ..., 0.783, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.372, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.676, 0.000, 0.000, 0.000,  ..., 0.895, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.372, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.372, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.357, 0.000,  ..., 0.619, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.367, 0.000, 0.000, 0.000,  ..., 0.847, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 1.230, 0.000, 0.000, 0.000,  ..., 0.365, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.199, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.367, 0.000, 0.000, 0.000,  ..., 0.847, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.854, 0.000, 0.516, 0.000,  ..., 1.856, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.516, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.136, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.478, 0.000,\n",
       "           0.000, 0.478, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.417, 0.000, 0.000, 0.000,  ..., 1.530, 0.000,\n",
       "           0.000, 0.000, 0.230, 0.417, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.714, 0.000, 0.420, 0.000,  ..., 0.420, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.498, 0.000, 0.000, 0.000,  ..., 1.077, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.369, 0.000, 0.000, 0.000, 0.000, 0.638, 0.000,  ..., 1.699, 0.000,\n",
       "           0.000, 0.000, 0.638, 0.369, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.498, 0.000, 0.000, 0.000,  ..., 1.077, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]]]),\n",
       " 'labeled': tensor([[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]], dtype=torch.float64),\n",
       " 'sizefactor': tensor([[238.928, 180.979, 152.858, 173.028, 152.858, 157.679, 150.860, 159.875],\n",
       "         [191.269, 153.167, 165.326, 153.167, 163.357, 173.028, 165.326, 159.875],\n",
       "         [175.417, 178.151, 167.858, 159.148, 178.513, 167.690, 147.855, 167.690],\n",
       "         [352.755, 339.793, 372.033, 389.102, 389.102, 389.460, 385.275, 382.810],\n",
       "         [472.497, 462.178, 420.197, 420.197, 494.025, 494.025, 493.527, 462.178],\n",
       "         [267.021, 136.110, 113.768, 157.136, 150.081, 167.901, 182.044, 159.851],\n",
       "         [309.806, 166.899, 167.690, 159.875, 152.909, 152.909, 172.537, 164.402],\n",
       "         ...,\n",
       "         [146.882, 180.979, 162.801, 158.750, 159.875, 167.690, 162.619, 159.148],\n",
       "         [130.542, 162.801, 145.486, 159.148, 167.690, 182.044, 157.679, 159.851],\n",
       "         [270.753, 153.167, 182.044, 164.402, 151.846, 163.746, 167.690, 164.402],\n",
       "         [249.887, 327.565, 336.825, 339.154, 363.084, 352.343, 348.540, 385.275],\n",
       "         [131.812, 165.032, 152.863, 152.858, 162.801, 157.650, 137.612, 159.851],\n",
       "         [236.384, 151.346, 159.851, 159.851, 169.993, 157.136, 175.608, 157.136],\n",
       "         [380.577, 178.151, 163.871, 382.483, 147.855, 180.511, 384.308, 180.511]]),\n",
       " 'batch': tensor([[[0],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[0],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]],\n",
       " \n",
       "         [[0],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[1],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[1],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[0],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[1],\n",
       "          [2],\n",
       "          [2],\n",
       "          [1],\n",
       "          [2],\n",
       "          [2],\n",
       "          [1],\n",
       "          [2]]]),\n",
       " 'combined_batch': tensor([[0, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [0, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [1, 2, 2, 2, 2, 2, 2, 2],\n",
       "         ...,\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [1, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [0, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [1, 2, 2, 1, 2, 2, 1, 2]]),\n",
       " 'celltypes': tensor([[[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[10],\n",
       "          [ 1],\n",
       "          [10],\n",
       "          [ 1],\n",
       "          [10],\n",
       "          [ 1],\n",
       "          [10],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 3],\n",
       "          [15],\n",
       "          [ 3],\n",
       "          [ 1],\n",
       "          [ 3],\n",
       "          [ 1],\n",
       "          [ 3],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [13],\n",
       "          [13],\n",
       "          [13],\n",
       "          [13],\n",
       "          [13]],\n",
       " \n",
       "         [[ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2]],\n",
       " \n",
       "         [[ 3],\n",
       "          [ 1],\n",
       "          [ 3],\n",
       "          [ 1],\n",
       "          [15],\n",
       "          [15],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 3],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 3],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 6],\n",
       "          [13],\n",
       "          [13],\n",
       "          [13],\n",
       "          [13],\n",
       "          [13],\n",
       "          [13],\n",
       "          [13]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[10],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [ 3],\n",
       "          [15],\n",
       "          [10],\n",
       "          [15]]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(t.train_loader))\n",
    "batch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
