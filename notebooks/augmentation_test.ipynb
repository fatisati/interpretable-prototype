{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f783dec6-7729-4b3d-8f89-2048f70a592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the current working directory (where the notebook is located)\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, os.pardir))\n",
    "\n",
    "# Set the parent directory as the working directory\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "# Optionally, you can add the parent directory to the system path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "# Print the current working directory to verify\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbc0120c-c9a7-4bf0-9694-42f014623295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['slurm-job-out', 'query_evaluation.ipynb', 'chatgpt-run.sbatch', 'swav_test.ipynb', 'data', 'cell_type_correlation.ipynb', 'swav', 'test.csv', 'barlow_twins_pytorch', 'pbmc3k_visualization.ipynb', 'swav_umaps_copy.ipynb', 'downstream.ipynb', 'new-run.sh', 'model_architecture.png', '.gitignore', 'environment.yml', 'run.sbatch', 'benchmark_models.ipynb', 'interpretable_ssl', 'notebooks', 'main.py', 'seacell_data.ipynb', 'main_swav.py', 'weighted-sampling-error.txt', 'new-run.sbatch', 'apex_test.py', 'plot_prototypes.ipynb', 'test.png', 'compare_ssl_results.ipynb', 'Untitled1.ipynb', 'error.txt', 'swav_test.py', 'prototypes.csv', 'main_scpoli.py', '__pycache__', 'evaluate_ssl.py', 'projector_test.ipynb', 'main_old.py', 'output.txt', 'cache', 'swav_umaps.ipynb', 'run.sh', 'results', 'model_architecture', 'runs', 'Untitled.ipynb', 'pbmc3k_visualization_backup.ipynb', 'shiny-out.txt', 'my_shinyapp', 'wandb', 'shiny-err.txt', 'marker_genes_test.ipynb', '.ipynb_checkpoints', 'immune.ipynb', 'ssl_benchmark_test.ipynb', 'hlca.ipynb', 'scib_metrics.ipynb', 'fatemeh_test.ipynb', '.git', 'simcrl_test.ipynb', 'weighted_sampling.ipynb']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbf7af6-0646-436e-92cf-f79bf3f95edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:In order to use the mouse gastrulation seqFISH datsets, please install squidpy (see https://github.com/scverse/squidpy).\n",
      "INFO:pytorch_lightning.utilities.seed:Global seed set to 0\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "  new_rank_zero_deprecation(\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: The `pytorch_lightning.loggers.base.rank_zero_experiment` is deprecated in v1.7 and will be removed in v1.9. Please use `pytorch_lightning.loggers.logger.rank_zero_experiment` instead.\n",
      "  return new_rank_zero_deprecation(*args, **kwargs)\n",
      "WARNING:root:In order to use sagenet models, please install pytorch geometric (see https://pytorch-geometric.readthedocs.io) and \n",
      " captum (see https://github.com/pytorch/captum).\n",
      "WARNING:root:mvTCR is not installed. To use mvTCR models, please install it first using \"pip install mvtcr\"\n",
      "WARNING:root:multigrate is not installed. To use multigrate models, please install it first using \"pip install multigrate\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'interpretable_ssl.trainers.swav' from '/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/interpretable_ssl/trainers/swav.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# Import the augmenter module\n",
    "from interpretable_ssl.augmenters import adata_augmenter\n",
    "# Import the Swav trainer module\n",
    "from interpretable_ssl.trainers import scpoli_trainer\n",
    "\n",
    "from interpretable_ssl.trainers import swav\n",
    "\n",
    "# Reload the modules to ensure you have the latest version\n",
    "importlib.reload(adata_augmenter)\n",
    "importlib.reload(scpoli_trainer)\n",
    "importlib.reload(swav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc393879-3fc3-4b66-90c3-00325c11d066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    }
   ],
   "source": [
    "swav_instance = swav.SwAV(\n",
    "    num_prototypes=300,\n",
    "    latent_dims=8,\n",
    "    batch_size=1024,\n",
    "    augmentation_type='cell_type',\n",
    "    epsilon=0.02,\n",
    "    cvae_loss_scaler=0,\n",
    "    prot_decoding_loss_scaler=0,\n",
    "    experiment_name='reproduce-try',\n",
    "    model_version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f48b9b6b-f649-4146-9152-70b9db7e7623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08/15/24 10:24:43 - 0:00:00 - ============ Initialized logger ============\n",
      "INFO - 08/15/24 10:24:43 - 0:00:00 - all_latent: None\n",
      "                                     augmentation_type: cell_type\n",
      "                                     base_lr: 4.8\n",
      "                                     batch_size: 1024\n",
      "                                     batch_size_version: 2\n",
      "                                     cell_type_key: cell_type\n",
      "                                     checkpoint_freq: 25\n",
      "                                     condition_key: study\n",
      "                                     crops_for_assign: [0, 1]\n",
      "                                     custom_cross_val: False\n",
      "                                     cvae_loss_scaler: 0\n",
      "                                     cvae_reg: 0\n",
      "                                     dataset: pbmc-immune\n",
      "                                     dataset_id: pbmc-immune\n",
      "                                     debug: False\n",
      "                                     default_values: {'dataset_id': 'pbmc-immune', 'model_name_version': 5.0, 'num_prototypes': 128, 'hidden_dim': 64, 'latent_dims': 8, 'batch_size_version': 2, 'batch_size': 512, 'fine_tuning_epochs': 100, 'custom_cross_val': False, 'description': '', 'experiment_name': '', 'condition_key': 'study', 'cell_type_key': 'cell_type', 'epochs': 300, 'linear_eval': False, 'only_eval': False, 'use_early_stopping': False, 'debug': False, 'dump_name_version': 4, 'nmb_crops': [8], 'augmentation_type': 'knn', 'size_crops': [224], 'min_scale_crops': [0.14], 'max_scale_crops': [1], 'crops_for_assign': [0, 1], 'temperature': 0.1, 'epsilon': 0.05, 'sinkhorn_iterations': 3, 'feat_dim': 8, 'queue_length': 0, 'epoch_queue_starts': 15, 'base_lr': 4.8, 'final_lr': 0, 'freeze_prototypes_niters': 313, 'wd': 1e-06, 'warmup_epochs': 10, 'start_warmup': 0, 'cvae_reg': 0, 'dist_url': 'env://', 'world_size': -1, 'rank': 0, 'local_rank': 0, 'workers': 10, 'checkpoint_freq': 25, 'use_fp16': False, 'sync_bn': 'pytorch', 'syncbn_process_group_size': 8, 'seed': 31, 'model': '', 'optimizer': '', 'lr_schedule': '', 'queue': None, 'train_loader': '', 'training_stats': '', 'device': 'cuda', 'freezable_prototypes': False, 'cvae_loss_scaler': 0.0001, 'prot_decoding_loss_scaler': 5, 'hidden_mlp': 1024, 'swav_dim': 64, 'use_projector': False, 'model_version': 2, 'train_decoder': False}\n",
      "                                     description: None\n",
      "                                     device: cuda\n",
      "                                     dist_url: env://\n",
      "                                     dump_checkpoints: /home/icb/fatemehs.hashemig/models//pbmc-immune/reproduce-try_num-prot-300_latent8-bs1024_aug-cell_type8_ep0.02_model-v1/checkpoints\n",
      "                                     dump_name_version: 4\n",
      "                                     dump_path: /home/icb/fatemehs.hashemig/models//pbmc-immune/reproduce-try_num-prot-300_latent8-bs1024_aug-cell_type8_ep0.02_model-v1\n",
      "                                     epoch_queue_starts: 15\n",
      "                                     epochs: 300\n",
      "                                     epsilon: 0.02\n",
      "                                     experiment_name: reproduce-try\n",
      "                                     feat_dim: 8\n",
      "                                     final_lr: 0\n",
      "                                     fine_tuning_epochs: 100\n",
      "                                     fold: None\n",
      "                                     freezable_prototypes: False\n",
      "                                     freeze_prototypes_niters: 313\n",
      "                                     hidden_dim: 64\n",
      "                                     hidden_mlp: 1024\n",
      "                                     input_dim: 4000\n",
      "                                     latent_dims: 8\n",
      "                                     linear_eval: False\n",
      "                                     local_rank: 0\n",
      "                                     lr_schedule: None\n",
      "                                     max_scale_crops: [1]\n",
      "                                     min_scale_crops: [0.14]\n",
      "                                     model: None\n",
      "                                     model_name_version: 5.0\n",
      "                                     model_version: 1\n",
      "                                     nmb_crops: [8]\n",
      "                                     nmb_prototypes: 300\n",
      "                                     num_prototypes: 300\n",
      "                                     only_eval: False\n",
      "                                     optimizer: None\n",
      "                                     prot_decoding_loss_scaler: 0\n",
      "                                     query: pbmc-immune\n",
      "                                     query_latent: None\n",
      "                                     queue: None\n",
      "                                     queue_length: 0\n",
      "                                     rank: 0\n",
      "                                     ref: pbmc-immune\n",
      "                                     ref_latent: None\n",
      "                                     seed: 31\n",
      "                                     sinkhorn_iterations: 3\n",
      "                                     size_crops: [224]\n",
      "                                     start_warmup: 0\n",
      "                                     swav_dim: 64\n",
      "                                     sync_bn: pytorch\n",
      "                                     syncbn_process_group_size: 8\n",
      "                                     temperature: 0.1\n",
      "                                     train_decoder: False\n",
      "                                     train_loader: None\n",
      "                                     training_stats: None\n",
      "                                     use_early_stopping: False\n",
      "                                     use_fp16: False\n",
      "                                     use_projector: False\n",
      "                                     use_projector_out: False\n",
      "                                     warmup_epochs: 10\n",
      "                                     wd: 1e-06\n",
      "                                     workers: 10\n",
      "                                     world_size: -1\n",
      "INFO - 08/15/24 10:24:43 - 0:00:00 - The experiment will be stored in /home/icb/fatemehs.hashemig/models//pbmc-immune/reproduce-try_num-prot-300_latent8-bs1024_aug-cell_type8_ep0.02_model-v1\n",
      "                                     \n",
      "\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:151: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby('conditions_combined').first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:156: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  self.obs_metadata_ = adata.obs.groupby(condition_keys).first()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli_model.py:164: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  adata.obs[cell_type_key][self.labeled_indices_]\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "INFO - 08/15/24 10:24:43 - 0:00:00 - Building data done with 29137 images loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dictionary:\n",
      " \tNum conditions: [3]\n",
      " \tEmbedding dim: [10]\n",
      "Encoder Architecture:\n",
      "\tInput Layer in, out and cond: 4000 64 10\n",
      "\tMean/Var Layer in/out: 64 8\n",
      "Decoder Architecture:\n",
      "\tFirst Layer in, out and cond:  8 64 10\n",
      "\tOutput Layer in/out:  64 4000 \n",
      "\n",
      "swav model with l2n init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 08/15/24 10:24:44 - 0:00:00 - SwavBase(\n",
      "                                       (scpoli_model): scpoli(\n",
      "                                         (embeddings): ModuleList(\n",
      "                                           (0): Embedding(3, 10, max_norm=1.0)\n",
      "                                         )\n",
      "                                         (encoder): Encoder(\n",
      "                                           (FC): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=4000, out_features=64, bias=True)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=64, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (mean_encoder): Linear(in_features=64, out_features=8, bias=True)\n",
      "                                           (log_var_encoder): Linear(in_features=64, out_features=8, bias=True)\n",
      "                                         )\n",
      "                                         (decoder): Decoder(\n",
      "                                           (FirstL): Sequential(\n",
      "                                             (L0): CondLayers(\n",
      "                                               (expr_L): Linear(in_features=8, out_features=64, bias=False)\n",
      "                                               (cond_L): Linear(in_features=10, out_features=64, bias=False)\n",
      "                                             )\n",
      "                                             (N0): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
      "                                             (A0): ReLU()\n",
      "                                             (D0): Dropout(p=0.05, inplace=False)\n",
      "                                           )\n",
      "                                           (HiddenL): Sequential()\n",
      "                                           (mean_decoder): Sequential(\n",
      "                                             (0): Linear(in_features=64, out_features=4000, bias=True)\n",
      "                                             (1): Softmax(dim=-1)\n",
      "                                           )\n",
      "                                         )\n",
      "                                       )\n",
      "                                       (prototypes): Linear(in_features=8, out_features=300, bias=False)\n",
      "                                     )\n",
      "INFO - 08/15/24 10:24:44 - 0:00:00 - Building model done.\n",
      "INFO - 08/15/24 10:24:44 - 0:00:00 - Building optimizer done.\n",
      "INFO - 08/15/24 10:24:44 - 0:00:00 - no mixed precision\n"
     ]
    }
   ],
   "source": [
    "swav_instance.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac84bf3f-45a2-4438-be7f-b98aaabf1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = swav_instance.train_ds\n",
    "# ds[0]['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f20a1393-757e-47e1-81d7-e7459e94f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:84: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [32768000], which does not match the required output shape [1024, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [32768000], which does not match the required output shape [1024, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [32768000], which does not match the required output shape [1024, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [32768000], which does not match the required output shape [1024, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [32768000], which does not match the required output shape [1024, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [32768000], which does not match the required output shape [1024, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [32768000], which does not match the required output shape [1024, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [32768000], which does not match the required output shape [1024, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [32768000], which does not match the required output shape [1024, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [32768000], which does not match the required output shape [1024, 8, 4000]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/trainers/scpoli/_utils.py:86: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1024, 8]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/aten/src/ATen/native/Resize.cpp:28.)\n",
      "  return torch.stack(batch, 0, out=out)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(swav_instance.train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a4c433-473a-4c4b-bf9d-b35418cbd3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swav model with l2n init\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m swav_instance\u001b[38;5;241m.\u001b[39mget_model()\n\u001b[1;32m      2\u001b[0m batch \u001b[38;5;241m=\u001b[39m swav_instance\u001b[38;5;241m.\u001b[39mmove_input_on_device(batch)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/ictstr01/home/icb/fatemehs.hashemig/codes/interpretable-ssl/interpretable_ssl/models/swav.py:21\u001b[0m, in \u001b[0;36mSwavBase.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m---> 21\u001b[0m     x, recon_loss, kl_loss, mmd_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscpoli_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection_head(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/scarches/models/scpoli/scpoli.py:137\u001b[0m, in \u001b[0;36mscpoli.forward\u001b[0;34m(self, x, batch, combined_batch, sizefactor, celltypes, labeled)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    130\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m     labeled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    136\u001b[0m ):   \n\u001b[0;32m--> 137\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m(batch[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])])\n\u001b[1;32m    138\u001b[0m     x_log \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m x)\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecon_loss \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/nn/modules/container.py:295\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())[idx])\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/miniconda3/envs/apex-env/lib/python3.12/site-packages/torch/nn/modules/container.py:285\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    283\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    287\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of range"
     ]
    }
   ],
   "source": [
    "model = swav_instance.get_model()\n",
    "batch = swav_instance.move_input_on_device(batch)\n",
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4fc7b6-ec01-4910-a11d-f3f9a826305a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[0.000, 0.000, 0.000, 1.142, 0.000, 0.000, 0.000,  ..., 1.347, 0.537,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.442, 0.000, 0.000, 0.000,  ..., 0.981, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.757, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.375, 0.000, 0.000, 0.000,  ..., 1.316, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.176, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.580, 0.572,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.327, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.982, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.854, 0.000, 0.000, 0.000,  ..., 1.308, 0.000,\n",
       "           0.000, 0.000, 1.308, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.031, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.599, 0.000,\n",
       "           0.000, 0.000, 0.599, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.955, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.955, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.955, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.955, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.955, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.926, 0.000, 0.000, 0.000,  ..., 1.190, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.565, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.607, 0.000, 0.000, 0.000,  ..., 0.607, 0.000,\n",
       "           0.000, 0.349, 0.000, 0.349, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.798, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.420, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.548, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.475],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.475]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 1.588, 0.000, 0.000, 0.000,  ..., 1.080, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 1.063, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.963, 0.000, 0.000, 0.000,  ..., 0.963, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 1.063, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.961, 0.000, 0.000,  ..., 1.442, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.963, 0.000, 0.000, 0.000,  ..., 0.963, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 1.026, 0.000, 0.000, 0.000,  ..., 1.522, 0.000,\n",
       "           1.026, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.473, 0.000, 0.000, 0.473, 0.473, 0.473, 0.000,  ..., 1.035, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.013, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.460, 0.000],\n",
       "          [0.000, 0.000, 0.000, 1.958, 0.000, 0.000, 0.000,  ..., 0.700, 0.000,\n",
       "           0.000, 0.700, 0.000, 0.700, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.736, 0.000, 0.000, 0.000,  ..., 0.736, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.303, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.787, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.541, 0.000, 0.000, 0.000,  ..., 2.009, 0.000,\n",
       "           0.000, 0.000, 0.541, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.988, 0.611, 0.000, 0.000,  ..., 1.652, 0.000,\n",
       "           0.000, 0.000, 0.611, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.898, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.468, 0.000, 0.000, 0.000,  ..., 1.852, 0.785,\n",
       "           0.000, 0.000, 0.468, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.988, 0.611, 0.000, 0.000,  ..., 1.652, 0.000,\n",
       "           0.000, 0.000, 0.611, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.716, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.988, 0.611, 0.000, 0.000,  ..., 1.652, 0.000,\n",
       "           0.000, 0.000, 0.611, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.961, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.795, 0.795, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.721, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.728, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.859, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.142, 0.000,\n",
       "           0.000, 0.726, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.422, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.354, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.354, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.565, 0.000,\n",
       "           0.000, 0.000, 0.565, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.616, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.721, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.657, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.753, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.593, 0.000, 0.000, 0.000,  ..., 2.208, 0.000,\n",
       "           0.000, 0.593, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 2.010, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.753, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.390, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 2.010, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.626, 0.000, 0.000, 0.000,  ..., 1.677, 0.000,\n",
       "           0.000, 0.000, 0.626, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 2.128, 0.000,\n",
       "           0.000, 0.000, 0.600, 0.600, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.619, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.633, 0.000,\n",
       "           0.000, 0.000, 0.633, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.633, 0.000,\n",
       "           0.000, 0.000, 0.633, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.735, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.065, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.653, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.973, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.010, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.989, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 2.042, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.749, 0.749, 0.000, 0.000, 0.000,  ..., 1.470, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.749],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.824, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.918, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.293, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.243, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.725, 0.000, 0.000, 0.000,  ..., 0.725, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.735, 0.000, 0.000, 0.000,  ..., 0.735, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.737, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.821, 0.000,\n",
       "           0.000, 0.000, 0.354, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.314, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.952, 0.000,\n",
       "           0.000, 0.000, 0.586, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.690, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.314, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.617, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.013, 0.000,\n",
       "           0.000, 1.013, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.903, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.170, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.617, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 1.090, 0.000],\n",
       "          [0.000, 0.000, 0.000, 1.438, 0.000, 0.000, 0.000,  ..., 0.000, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 1.710, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]],\n",
       " \n",
       "         [[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.754, 0.000,\n",
       "           0.754, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.933, 0.571, 0.000, 0.571, 0.000,  ..., 1.197, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.571],\n",
       "          [0.000, 0.000, 0.000, 0.820, 0.000, 0.000, 0.000,  ..., 1.264, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000,  ..., 0.867, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.811, 0.000, 0.485, 0.000,  ..., 1.558, 0.000,\n",
       "           0.485, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.891, 0.891, 0.000, 0.000,  ..., 1.671, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.760, 0.000, 0.000, 0.000,  ..., 0.996, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "          [0.000, 0.000, 0.000, 0.820, 0.000, 0.000, 0.000,  ..., 1.264, 0.000,\n",
       "           0.000, 0.000, 0.000, 0.000, 0.000]]]),\n",
       " 'labeled': tensor([[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]], dtype=torch.float64),\n",
       " 'sizefactor': tensor([[222.605, 229.564, 186.162, 245.109, 201.889, 209.500, 194.728, 203.309],\n",
       "         [242.030, 223.837, 242.837, 233.689, 239.822, 263.315, 240.473, 240.473],\n",
       "         [298.557, 316.715, 347.894, 292.039, 316.932, 319.613, 329.455, 329.455],\n",
       "         [211.492, 233.781, 245.879, 214.690, 233.781, 214.690, 244.505, 245.879],\n",
       "         [246.800, 300.274, 280.657, 279.141, 239.727, 276.246, 269.151, 270.912],\n",
       "         [441.554, 440.959, 443.357, 458.293, 440.959, 427.352, 440.959, 426.314],\n",
       "         [243.619, 298.449, 283.016, 298.028, 316.061, 265.346, 226.354, 255.713],\n",
       "         ...,\n",
       "         [135.380, 112.402, 147.152, 152.858, 143.786, 136.739, 148.422, 125.391],\n",
       "         [174.496, 213.524, 207.212, 174.496, 187.695, 207.212, 175.513, 178.773],\n",
       "         [137.764, 154.885, 154.885, 140.008, 137.117, 131.483, 147.630, 127.564],\n",
       "         [140.490, 159.467, 177.439, 155.980, 157.010, 157.712, 194.516, 166.275],\n",
       "         [149.432, 151.891, 167.838, 157.798, 174.400, 168.677, 171.197, 174.400],\n",
       "         [224.802, 251.811, 231.519, 207.843, 224.802, 198.289, 187.992, 180.282],\n",
       "         [288.923, 304.920, 270.375, 262.496, 316.073, 291.681, 329.204, 270.375]]),\n",
       " 'batch': tensor([[[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]],\n",
       " \n",
       "         [[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2],\n",
       "          [2]],\n",
       " \n",
       "         [[0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0],\n",
       "          [0]],\n",
       " \n",
       "         [[1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1],\n",
       "          [1]]]),\n",
       " 'combined_batch': tensor([[2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         ...,\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [2, 2, 2, 2, 2, 2, 2, 2],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'celltypes': tensor([[[ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 5],\n",
       "          [ 5],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 4],\n",
       "          [ 4],\n",
       "          [ 4],\n",
       "          [ 4],\n",
       "          [ 4],\n",
       "          [ 4],\n",
       "          [ 4],\n",
       "          [ 4]],\n",
       " \n",
       "         [[ 5],\n",
       "          [ 5],\n",
       "          [ 5],\n",
       "          [ 5],\n",
       "          [ 5],\n",
       "          [ 5],\n",
       "          [ 5],\n",
       "          [ 5]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2],\n",
       "          [ 2]],\n",
       " \n",
       "         [[ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6],\n",
       "          [ 6]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 3],\n",
       "          [ 1],\n",
       "          [ 1]],\n",
       " \n",
       "         [[15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [15],\n",
       "          [ 3]],\n",
       " \n",
       "         [[ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3],\n",
       "          [ 3]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 5]],\n",
       " \n",
       "         [[ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1],\n",
       "          [ 1]]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb5bf19-f80f-4ad5-97e8-5ed98d7667fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<interpretable_ssl.augmenters.adata_augmenter.MultiCropsDataset at 0x7fdf09376e40>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self = swav_instance\n",
    "self.train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7bf37ef-7f78-46e9-845f-26f14a27f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "self = self.train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be795d2f-488e-41a6-b2b1-ade26835034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You’re trying to run this on 4000 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
      "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/fatemehs.hashemig/miniconda3/envs/apex-env/lib/python3.12/site-packages/anndata/_core/anndata.py:430: FutureWarning: The dtype argument is deprecated and will be removed in late 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sc.pp.neighbors(self.adata, n_neighbors=self.k_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f135600-4f5c-43f5-897e-bd0444138ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_graph = self.adata.obsp[\"distances\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "58381363-593f-42bc-b076-96abdf3fb5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_graph[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ac710-0431-492b-b847-d8c6462ad140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scanpy_knn_graph(self):\n",
    "    \"\"\"\n",
    "    Build a k-nearest neighbors graph using scanpy.pp.neighbors and format it \n",
    "    to match the output of build_knn_graph.\n",
    "    \"\"\"\n",
    "    sc.pp.neighbors(self.adata, n_neighbors=self.k_neighbors)\n",
    "    \n",
    "    # Extract the kNN graph information\n",
    "    knn_graph = self.adata.obsp[\"distances\"].copy()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2db08c7a-7dd1-4ab5-aec1-bb44e017e368",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Indices where the matrix is non-zero (indicating a connection)\n",
    "indices = []\n",
    "distances = []\n",
    "\n",
    "for i in range(knn_graph.shape[0]):\n",
    "    # Find non-zero entries in the sparse matrix for row i\n",
    "    nonzero_indices = knn_graph[i].nonzero()[1]\n",
    "    nonzero_distances = knn_graph[i, nonzero_indices].toarray().flatten()\n",
    "\n",
    "    # Store these in the indices and distances lists\n",
    "    indices.append(nonzero_indices)\n",
    "    distances.append(nonzero_distances)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "indices = np.array(indices)\n",
    "distances = np.array(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0c96efd6-585c-4529-83d8-5f7010da2ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29137, 9)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a83ce447-6a0c-4f95-af4c-35e848bca151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29137, 9)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ba140bd0-cbfa-4b66-8656-ff07513667e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i, d = self.build_knn_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a13e3557-6317-470c-aca7-a0e184a37255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29137, 11), (29137, 11))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.shape, d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4f4d2b2a-871b-4e4b-8848-6d39be5dfcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 20703, 25704, 22386, 26553, 24706, 24849, 25953, 22725,\n",
       "       24108, 23015])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4728c2a-9d09-43c0-ac7f-a13f3cf24011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
